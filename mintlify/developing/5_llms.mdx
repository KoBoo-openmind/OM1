---
title: LLMs
description: "LLM Integration"
---

OM1's LLM integration is intended to make it easy to (1) send `input` information to LLMs and then (2) route LLM responses to various system `actions`, such as `speak` and `move`. The system provides a standardized interface for communicating with many different LLM endpoints from all the major providers including Anthropic, Google, DeepSeek, and OpenAI. The plugins handle authentication, API communication, prompt formatting, response parsing, and conversation history management.

## LLM Plugin Examples

LLM plugins are located in `src/llm/plugins`: [**Code**](https://github.com/openmind/OM1/tree/main/src/llm/plugins).

## How it Works - Single-Agent LLM Integration

For testing and introductory educational purposes, we integrate with multiple language models (LLMs) to provide chat completion via a `post /{provider}/chat/completions` endpoint. Each LLM plugin takes fused input data (the `prompt`) and sends it to an LLM (or a system of LLMs), and then waits for the response. The response is then parsed and provided to `runtime/cortex.py` for distribution to the system actions:

```python
response = await self._client.beta.chat.completions.parse(
    model=self._config.model,
    messages=[*messages, {"role": "user", "content": prompt}],
    response_format=self._output_model,
    timeout=self._config.timeout,
)

message_content = response.choices[0].message.content
parsed_response = self._output_model.model_validate_json(message_content)

return parsed_response
```

The standard `pydantic` output model is defined in `src/llm/output_model.py`.

## LLM Configuration

```bash
  "cortex_llm": {
    "type": "OpenAILLM",    // The class name of the LLM plugin you wish to use
    "config": {
      "base_url": "",       // Optional: URL of the LLM endpoint
      "agent_name": "Iris", // Optional: Name of the agent
      "history_length": 10  // The number of input->action cycles to provide to the LLM as historical context 
    }
  }
```

# Multi-Agent LLM Integration (`post /agent`)

The Multi-Agent endpoint (accessed via `post /api/core/agent`) utilizes a collaborative system of specialized agents to process complex robotics tasks.

## Agent Architecture

The system employs four primary agents that work together:

- **Navigation Agent**: Processes spatial and movement-related tasks
- **Perception Agent**: Handles sensory input analysis and environmental understanding
- **RAG Agent**: Provides retrieval-augmented generation capabilities using the user's knowledge base
- **Team Agent**: Synthesizes outputs from all agents into a unified response

## API Endpoint

```python
@api_core_agent.route("/agent", methods=["POST"])
@flexible_api_auth(limit=0, period=60)
```

### Request Format

```json
{
    "model": "gpt-4.1-nano",
    "system_prompt": "System instructions for the agents",
    "inputs": "User input or sensor data",
    "available_actions": ["move", "speak", "grab"],
    "response_format": {}  // Optional structured output schema
}
```

### Response Structure

```json
{
    "content": "Synthesized response from team agent",
    "model": "gpt-4.1-nano",
    "agent_contents": {
        "team_agent": "Team agent output",
        "navigation_agent": "Navigation agent output",
        "perception_agent": "Perception agent output",
        "rag_agent": "RAG agent context"
    },
    "conversation_id": "unique-conversation-id",
    "usage": {
        "team_agent": {},
        "navigation_agent": {},
        "perception_agent": {},
        "rag_agent": {}
    },
    "duration": {
        "team_agent": 0.5,
        "navigation_agent": 0.3,
        "perception_agent": 0.4,
        "rag_agent": 0.2
    },
    "total_duration": 1.4
}
```

## Supported Models

```python
SUPPORTED_MODELS = ["gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"]
```

## Medical Agent Variant

A specialized endpoint for medical applications:

```python
@api_core_agent.route("/agent/medical", methods=["POST"])
```

This variant uses:
- **Verifier Agent**: Validates medical information
- **Questioner Agent**: Manages medical consultation flow

## Memory Management

The system includes memory capabilities:

```python
@api_core_agent.route("/agent/memory", methods=["DELETE"])
```

- Session-based memory storage via API keys
- Graph memory integration using Zep
- Conversation history tracking

## RAG Integration

The RAG agent connects to the knowledge base system (`/api/core/rag/*` endpoints) to:
- Retrieve relevant documents during agent processing
- Provide context-aware responses
- Access user-uploaded documents and files

## Getting Started

To try out the multi-agent system:

```bash
uv run src/run.py multiagent
```

## Technical Details

The multi-agent system:
- Processes navigation, perception, and RAG queries in parallel using `asyncio.gather()`
- Sends results to the team agent for synthesis
- Returns comprehensive response with individual agent outputs
- Tracks usage and duration metrics for each agent