# syntax=docker/dockerfile:1.6

########## uv binary stage (safer & reproducible) ##########
FROM --platform=linux/arm64 ghcr.io/astral-sh/uv:latest AS uvbin

########## main image ##########
FROM nvcr.io/nvidia/l4t-jetpack:r36.4.0
SHELL ["/bin/bash","-lc"]

# ---- Environment ----
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONNOUSERSITE=1 \
    CUDA_ROOT=/usr/local/cuda
ENV PATH="${CUDA_ROOT}/bin:${PATH}" \
    LD_LIBRARY_PATH="/usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu/tegra:${CUDA_ROOT}/lib64:${LD_LIBRARY_PATH}"

# ---- Base OS deps ----
RUN apt-get update && apt-get install -y \
    python3.10 python3.10-venv python3.10-dev \
    build-essential cmake pkg-config git curl ca-certificates \
    libv4l-dev v4l-utils ffmpeg libboost-all-dev \
    libasound2-dev portaudio19-dev libssl-dev vim \
 && rm -rf /var/lib/apt/lists/*

# ---- TensorRT Python from L4T repo ----
RUN apt-get update && apt-get install -y --no-install-recommends \
      python3-tensorrt \
      python3-libnvinfer python3-libnvinfer-dev \
      tensorrt nvidia-tensorrt-dev || true \
 && rm -rf /var/lib/apt/lists/*

# ---- uv binary from stage ----
COPY --from=uvbin /uv /uvx /usr/local/bin/

# ---- (Optional) CycloneDDS build ----
# Comment this block out if DDS is not needed.
WORKDIR /app
RUN git clone --branch releases/0.10.x https://github.com/eclipse-cyclonedds/cyclonedds
WORKDIR /app/cyclonedds/build
RUN cmake .. -DCMAKE_INSTALL_PREFIX=../install -DBUILD_EXAMPLES=ON \
 && cmake --build . --target install
ENV CYCLONEDDS_HOME=/app/cyclonedds/install \
    CMAKE_PREFIX_PATH=/app/cyclonedds/install

# ---- App code ----
WORKDIR /app/OM1
COPY . .
# Only try submodules if this is actually a git worktree and has .gitmodules
RUN if git rev-parse --is-inside-work-tree >/dev/null 2>&1 && [ -f .gitmodules ]; then \
      git submodule update --init --recursive; \
    else \
      echo "Skipping submodules (no .git/.gitmodules in build context)"; \
    fi

# ---- Isolated venv (do NOT expose system site-packages) ----
RUN python3 -m venv .venv
ENV VIRTUAL_ENV=/app/OM1/.venv
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Link ONLY TensorRT into the venv (match current Python X.Y)
RUN python3 - <<'PY'
import os, sys, site
venv_site = site.getsitepackages()[0]
ver = f"{sys.version_info.major}.{sys.version_info.minor}"
candidates = [
    f"/usr/lib/python{ver}/dist-packages/tensorrt",
    f"/usr/local/lib/python{ver}/dist-packages/tensorrt",
    "/usr/lib/python3/dist-packages/tensorrt",
]
src = next((p for p in candidates if os.path.exists(p)), None)
if not src:
    raise SystemExit("ERROR: tensorrt not found in system dist-packages.")
dst = os.path.join(venv_site, "tensorrt")
if not os.path.exists(dst):
    os.symlink(src, dst)
print("linked", dst, "->", src)
PY

# ---- PyCUDA build (NumPy first, then compile) ----
RUN set -euo pipefail; \
    python -m pip install -U pip setuptools wheel packaging \
      "numpy==2.1.0" "mako>=1.3" "pytools>=2024.1" "pybind11>=2.10"; \
    export CUDA_ROOT="${CUDA_ROOT:-/usr/local/cuda}"; \
    NUMPY_INC="$(python -c 'import numpy as np, sys; sys.stdout.write(np.get_include())')" ; \
    export CFLAGS="${CFLAGS:-} -I${CUDA_ROOT}/include -I${NUMPY_INC}"; \
    export CXXFLAGS="${CXXFLAGS:-} -I${CUDA_ROOT}/include"; \
    export LDFLAGS="${LDFLAGS:-} -L${CUDA_ROOT}/lib64"; \
    python -m pip uninstall -y pycuda || true; \
    python -m pip install --no-cache-dir --no-binary=:all: --no-build-isolation "pycuda>=2024.1"

# ---- Entry point: default no-sync; optional first-time setup & toggles ----
# Behavior:
# - If .venv missing at runtime: print "Creating virtualenv..." and create it, link tensorrt, then sync deps once.
# - If .venv exists: print "Reusing existing virtualenv...", and only sync when OM1_SYNC=1.
# - OM1_EXTRAS=dds -> include that extras group in sync.
# - OM1_UPGRADE_OM1_MODULES=1 -> bump that package (e.g., fetch branch HEAD) before sync.
RUN <<'BASH'
set -e
cat >/entrypoint.sh <<'SH'
#!/usr/bin/env bash
set -euo pipefail

cd /app/OM1

link_trt_into_venv() {
  python - <<'PY'
import os, sys, site
venv_site = site.getsitepackages()[0]
dst = os.path.join(venv_site, "tensorrt")
if os.path.exists(dst):
    print("tensorrt already linked in venv:", dst)
    raise SystemExit(0)
ver = f"{sys.version_info.major}.{sys.version_info.minor}"
candidates = [
    f"/usr/lib/python{ver}/dist-packages/tensorrt",
    f"/usr/local/lib/python{ver}/dist-packages/tensorrt",
    "/usr/lib/python3/dist-packages/tensorrt",
]
src = next((p for p in candidates if os.path.exists(p)), None)
if not src:
    raise SystemExit("ERROR: tensorrt not found in system dist-packages.")
os.symlink(src, dst)
print("linked", dst, "->", src)
PY
}

if [[ ! -d ".venv" ]]; then
  echo ">> Creating virtualenv and installing deps..."
  python3 -m venv .venv
  export VIRTUAL_ENV="/app/OM1/.venv"
  export PATH="$VIRTUAL_ENV/bin:$PATH"
  link_trt_into_venv

  ARGS=(sync --active)
  if [[ -n "${OM1_EXTRAS:-}" ]]; then
    ARGS+=(--extra "${OM1_EXTRAS}")
  fi
  if [[ "${OM1_UPGRADE_OM1_MODULES:-0}" == "1" ]]; then
    uv lock --upgrade-package om1-modules || true
  fi
  uv "${ARGS[@]}"

else
  echo ">> Reusing existing virtualenv at /app/OM1/.venv"
  if [[ "${OM1_SYNC:-0}" == "1" ]]; then
    ARGS=(sync --active)
    if [[ -n "${OM1_EXTRAS:-}" ]]; then
      ARGS+=(--extra "${OM1_EXTRAS}")
    fi
    if [[ "${OM1_UPGRADE_OM1_MODULES:-0}" == "1" ]]; then
      uv lock --upgrade-package om1-modules || true
    fi
    uv "${ARGS[@]}"
  fi
fi

# Show which python is active
python -c 'import sys; print("Python:", sys.executable)'

# Run the app (lock-free by default; uv will resolve/install if needed)
exec uv run --active src/run.py "$@"
SH
chmod +x /entrypoint.sh
BASH

# If you want to run uv run --active src/run.py spot by default
# ENTRYPOINT ["/entrypoint.sh"]
# CMD ["spot"]

# First run: docker build -t om1:jetson-r36.4 -f Dockerfile.jetson .
# 1. Fast build: docker run -it --rm --runtime nvidia om1:jetson-r36.4
# 2. First time build venv need dds: docker run -it --rm --runtime nvidia -e OM1_EXTRAS=dds om1:jetson-r36.4 bash
# 3. Next time want to refresh the dependencies with dds: docker run -it --rm --runtime nvidia -e OM1_SYNC=1 -e OM1_EXTRAS=dds om1:jetson-r36.4 bash
# 4. Upgrade om1-modules to newest then sync:
#     docker run -it --rm --runtime nvidia -e OM1_SYNC=1 -e OM1_UPGRADE_OM1_MODULES=1  om1:jetson-r36.4 bash
# Usual Usage w/ camera etc..
# docker run -it --rm   --runtime nvidia   --network host   --shm-size 1g   -v /tmp/argus_socket:/tmp/argus_socket   --device /dev/video0   om1:jetson-r36.4 bash
# -t: interactive with your terminal --rm: auto remove container on exit --runtime nvidia" enable NVIDIA runtime (GPU/Jetson passthrough) --network host: use host networking 
# --shm-size 1g: enlarge /dev/shm for pytorch/numpy/ROS multiprocess -v /tmp/argus_socket:....: expose Jetson camera argus socket to container
# --device /dev/video0: pass a V4L2 camera (repeat for more devices).